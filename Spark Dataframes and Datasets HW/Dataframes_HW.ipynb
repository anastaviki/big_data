{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa35e62-2b3c-4565-8a88-691593250b9c",
   "metadata": {},
   "source": [
    "In this homework we are going to use Jupyter/Databricks Notebooks. Please complete the tasks within one notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64585c40-85d7-4bcc-afd2-3ae64cb93393",
   "metadata": {},
   "source": [
    "**TASK 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0578e13-1206-4a75-863f-9146c747d5b2",
   "metadata": {},
   "source": [
    "1.\tPlease run your code from previous homework, which creates a sellers_rdd, header, filter a sellers_rdd without header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "11e20a16-51b4-4105-a6c3-0bfb655c0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9d3394e7-fd83-40be-8a98-247ecb945a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark: Dataframes_HW\").config(\"spark.app.name\", \"Spark: Dataframes_HW\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0750e401-bfad-4ebc-8e04-16fc031a3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seller_id :  <class 'str'>\n",
      "seller_name :  <class 'str'>\n",
      "daily_target :  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "relative_path = os.path.join(current_directory, 'RDD_HW')\n",
    "file_path = relative_path +'/sellers.csv'\n",
    "sellers_rdd = sc.textFile(file_path).map(lambda x: x.split(\",\"))\n",
    "sellers_rdd_header = sellers_rdd.first()\n",
    "sellers_rdd = sellers_rdd.filter(lambda x: x != sellers_rdd_header)\n",
    "data = sellers_rdd.take(10)\n",
    "i = 0\n",
    "for item in sellers_rdd.first():\n",
    "    print(sellers_rdd_header[i] ,': ', type(item))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba025f7c-0b9d-41b1-af32-0b29634b6b47",
   "metadata": {},
   "source": [
    "2.\tCreate a DataFrame variable sellers_df_test using toDF() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2c5d387c-a92a-49a8-bfaf-01762661d1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[seller_id: string, seller_name: string, daily_target: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sellers_df_test = sellers_rdd.toDF([\"seller_id\", \"seller_name\", \"daily_target\"])\n",
    "display(sellers_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450379d1-b49d-49c5-bdb4-800d8596220b",
   "metadata": {},
   "source": [
    "3.\tRun the following code in order to change data types \r\n",
    "sellers_rdd = sellers_rdd.map(lambda x: (int(x[0]),x[1],int(x[2])))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6178f807-1253-42a1-8556-96fd2b111117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seller_id :  <class 'int'>\n",
      "seller_name :  <class 'str'>\n",
      "daily_target :  <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "sellers_rdd = sellers_rdd.map(lambda x: (int(x[0]), x[1], int(x[2])))\n",
    "i = 0\n",
    "for item in sellers_rdd.first():\n",
    "    print(sellers_rdd_header[i] ,': ',type(item))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1654ec7-2b87-498d-a45f-bca10069f38e",
   "metadata": {},
   "source": [
    "4.\tRecreate a DataFrame variable sellers_df using createDataFrame() function. For this task you need to create a schema. Review the dataset in order to choose correct data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "63561edf-a230-448a-8de0-8643b2cca95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[seller_id: int, seller_name: string, daily_target: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"seller_id\", IntegerType(), True),\n",
    "    StructField(\"seller_name\", StringType(), True),\n",
    "    StructField(\"daily_target\", IntegerType(), True)\n",
    "])\n",
    "sellers_df = spark.createDataFrame(sellers_rdd, schema=schema)\n",
    "display(sellers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89461c4c-541e-43f7-b30b-4bbb39c4ad44",
   "metadata": {},
   "source": [
    "5.\tCreate a Pandas dataframe sellers_df_pd  from Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5292e266-c163-4075-bb59-670bad26ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   seller_id seller_name  daily_target\n",
      "0          0    seller_0        100000\n",
      "1          1    seller_1         83478\n",
      "2          2    seller_2         94114\n",
      "3          3    seller_3         50299\n",
      "4          4    seller_4         72654\n",
      "5          5    seller_5         28862\n",
      "6          6    seller_6         61878\n",
      "7          7    seller_7         72047\n",
      "8          8    seller_8         54715\n",
      "9          9    seller_9         82824\n"
     ]
    }
   ],
   "source": [
    "sellers_df_pd  = sellers_df.toPandas()\n",
    "print(sellers_df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ca1c9-a5e2-4082-b1df-a83a94837a3f",
   "metadata": {},
   "source": [
    "6.\tTake first 5 rows of sellers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c10826d8-20a9-4da3-8275-e0a794de5972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(seller_id=0, seller_name='seller_0', daily_target=100000)\n",
      "Row(seller_id=1, seller_name='seller_1', daily_target=83478)\n",
      "Row(seller_id=2, seller_name='seller_2', daily_target=94114)\n",
      "Row(seller_id=3, seller_name='seller_3', daily_target=50299)\n",
      "Row(seller_id=4, seller_name='seller_4', daily_target=72654)\n"
     ]
    }
   ],
   "source": [
    "first_5_rows = sellers_df.head(5)\n",
    "for row in first_5_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c74a8-33ca-49dc-8fbd-0e5ed168352e",
   "metadata": {},
   "source": [
    "7.\tReturn a schema and dtypes of sellers_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "99c1c484-8dc1-4ba5-9cd0-7e4052a8e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of sellers_df:\n",
      "root\n",
      " |-- seller_id: integer (nullable = true)\n",
      " |-- seller_name: string (nullable = true)\n",
      " |-- daily_target: integer (nullable = true)\n",
      "\n",
      "Data Types of columns in sellers_df:\n",
      "Column 'seller_id' has data type 'int'\n",
      "Column 'seller_name' has data type 'string'\n",
      "Column 'daily_target' has data type 'int'\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema of sellers_df:\")\n",
    "sellers_df.printSchema()\n",
    "print(\"Data Types of columns in sellers_df:\")\n",
    "for field, data_type in sellers_df.dtypes:\n",
    "    print(f\"Column '{field}' has data type '{data_type}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6f354-22e6-439f-b469-52b20b748bfc",
   "metadata": {},
   "source": [
    "**TASK 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedb925-057c-4ded-ad88-8a5dcfa30cbc",
   "metadata": {},
   "source": [
    "1.\tUpload data from products.csv to products_df Dataframe variable and sales.csv to sales_df using the spark.read.csv() function. Please take a look on the header parameter while data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "53eb69f1-d769-4503-b034-351c57029193",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = spark.read.csv(os.path.join(relative_path, 'products.csv'), header=True)\n",
    "sales_df = spark.read.csv(os.path.join(relative_path, 'sales.csv'), header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543cd9f3-c1c7-4e60-8067-8b11f0121af9",
   "metadata": {},
   "source": [
    "2.\tShow the content of products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "97027539-ec51-43bb-8196-f65130ed3967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         0|   product_0|   22|\n",
      "|         1|   product_1|   35|\n",
      "|         2|   product_2|  146|\n",
      "|         3|   product_3|   17|\n",
      "|         4|   product_4|   66|\n",
      "|         5|   product_5|   31|\n",
      "|         6|   product_6|  127|\n",
      "|         7|   product_7|  116|\n",
      "|         8|   product_8|  121|\n",
      "|         9|   product_9|   98|\n",
      "|        10|  product_10|   54|\n",
      "|        11|  product_11|   25|\n",
      "|        12|  product_12|  125|\n",
      "|        13|  product_13|    8|\n",
      "|        14|  product_14|  100|\n",
      "|        15|  product_15|  111|\n",
      "|        16|  product_16|    1|\n",
      "|        17|  product_17|  115|\n",
      "|        18|  product_18|   69|\n",
      "|        19|  product_19|   59|\n",
      "+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e93b5-a5cc-40a1-8457-8bf48e7a6e4d",
   "metadata": {},
   "source": [
    "3.\tShow the summary statistic of products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0ab9bbf8-5e40-42a6-ba64-c320e09bc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------+-----------------+\n",
      "|summary|       product_id| product_name|            price|\n",
      "+-------+-----------------+-------------+-----------------+\n",
      "|  count|           100000|       100000|           100000|\n",
      "|   mean|          49999.5|         null|         75.60715|\n",
      "| stddev|28867.65779668774|         null|43.33316001060901|\n",
      "|    min|                0|    product_0|                1|\n",
      "|    max|            99999|product_99999|               99|\n",
      "+-------+-----------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff525e35-4fe1-4e36-93d2-08b78f0e0e0f",
   "metadata": {},
   "source": [
    "4.\tPrint the physical and logical plans of products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6d789cf2-6965-4fbb-8f04-4efe2a715815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Plan:\n",
      "== Physical Plan ==\n",
      "FileScan csv [product_id#2244,product_name#2245,price#2246] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/RDD_HW/products.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,product_name:string,price:string>\n",
      "\n",
      "\n",
      "\n",
      "Logical Plan and Physical Plan:\n",
      "== Parsed Logical Plan ==\n",
      "Relation [product_id#2244,product_name#2245,price#2246] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "product_id: string, product_name: string, price: string\n",
      "Relation [product_id#2244,product_name#2245,price#2246] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [product_id#2244,product_name#2245,price#2246] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "FileScan csv [product_id#2244,product_name#2245,price#2246] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/RDD_HW/products.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:string,product_name:string,price:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Physical Plan:\")\n",
    "products_df.explain()\n",
    "print(\"\\nLogical Plan and Physical Plan:\")\n",
    "products_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d0596-f80a-45a0-8177-e3f31d5896e3",
   "metadata": {},
   "source": [
    "5.\tCount a number of distinct rows of products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "9c7ceb96-baf0-438d-bff8-157ddea4c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count a number of distinct rows of products_df: 100000\n"
     ]
    }
   ],
   "source": [
    "print('Count a number of distinct rows of products_df:',products_df.distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384be3e-07c7-4620-b97b-824b349641e1",
   "metadata": {},
   "source": [
    "6.\tTransform a dataframe to RDD products_rdd variable using products_df.rdd method and print a number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7ee4f895-4535-49b3-9535-552ce09f70ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in products_rdd: 1\n"
     ]
    }
   ],
   "source": [
    "products_rdd = products_df.rdd\n",
    "num_partitions = products_rdd.getNumPartitions()\n",
    "print(\"Number of partitions in products_rdd:\", num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51e7d3-e812-4501-bce6-e634bf82c2c6",
   "metadata": {},
   "source": [
    "7.\tFilter products_df where price > 100 and save a dataframe to the products_df_more_than_100.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0c98e89a-4976-425c-a3f3-fdab1b3a44aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_products_df = products_df.filter(products_df[\"price\"] > 100)\n",
    "output_file_name = \"products_df_more_than_100\"\n",
    "output_file_path = os.path.join(relative_path, output_file_name)\n",
    "filtered_products_df.coalesce(1).write.format(\"com.databricks.spark.csv\").mode(\"overwrite\").option(\"header\", \"true\").csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bfff9189-ac5c-422b-85bf-12a28c16b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file pattern to match\n",
    "file_pattern = 'part*.csv'\n",
    "wildcard_pattern = os.path.join(output_file_path, file_pattern)\n",
    "# Use glob to find matching files\n",
    "matching_files = glob.glob(wildcard_pattern)\n",
    "# rename csv file\n",
    "if len(matching_files) == 1:\n",
    "    new_file_path = os.path.join(output_file_path, 'products_df_more_than_100.csv')\n",
    "    os.rename(matching_files[0], new_file_path)\n",
    "else:\n",
    "    print('Something goes wrong, result possibly saved more than in 1 file or result did non save')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a8cc3-7329-43ba-a336-11e01877c870",
   "metadata": {},
   "source": [
    "8.\tSave products_df to JSON and Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8f312ee6-2648-4a4f-bc45-a552479bd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output_file_path = relative_path+\"/products_df_json\"\n",
    "parquet_output_file_path = relative_path+\"/products_df_parquet\"\n",
    "products_df.write.mode(\"overwrite\").json(json_output_file_path)\n",
    "products_df.write.mode(\"overwrite\").parquet(parquet_output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21ed5d2-7824-4585-9bf6-46beabf9d82f",
   "metadata": {},
   "source": [
    "9.\tUpload the JSON file from the step above to the products_df_from_json Dataframe variable and show the content of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "72ce93b2-b5e2-4b5b-8a1e-b147b3bb9d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+\n",
      "|price|product_id|product_name|\n",
      "+-----+----------+------------+\n",
      "|   22|         0|   product_0|\n",
      "|   35|         1|   product_1|\n",
      "|  146|         2|   product_2|\n",
      "|   17|         3|   product_3|\n",
      "|   66|         4|   product_4|\n",
      "|   31|         5|   product_5|\n",
      "|  127|         6|   product_6|\n",
      "|  116|         7|   product_7|\n",
      "|  121|         8|   product_8|\n",
      "|   98|         9|   product_9|\n",
      "|   54|        10|  product_10|\n",
      "|   25|        11|  product_11|\n",
      "|  125|        12|  product_12|\n",
      "|    8|        13|  product_13|\n",
      "|  100|        14|  product_14|\n",
      "|  111|        15|  product_15|\n",
      "|    1|        16|  product_16|\n",
      "|  115|        17|  product_17|\n",
      "|   69|        18|  product_18|\n",
      "|   59|        19|  product_19|\n",
      "+-----+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df_from_json = spark.read.json(json_output_file_path)\n",
    "products_df_from_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "572f3850-ccb7-4ba9-b966-d18f50187c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead5636-f40d-4eaf-b058-c4aa39b3c1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
